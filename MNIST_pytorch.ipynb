{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image recognition with MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST dataset is one of the most well studied datasets for image recognition.\n",
    "\n",
    "**Objective**\n",
    "Experiment with various deep learning techniques to see it's effects and understand what they mean.\n",
    "\n",
    "I will be iteratively trying something and see it's effects on train accuracy and validation(test) accuracy. This is just an exercise to see how tinkering with each cog would work, and this approach is advisable only for learning purpose. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model v1**\n",
    "\n",
    "Made mistake of comparing training set for validataion instead of test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "dtype = torch.float\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in data\n",
    "\n",
    "Let's load in the data. Thankfully torchvision library has inbuilt functions to download the data and load it in.\n",
    "We will be doing only minimal transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train' : transforms.Compose(\n",
    "    [transforms.RandomRotation(10),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ]),\n",
    "    'val' : transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "}\n",
    "\n",
    "img_datasets = {'train': datasets.MNIST(root='./data', train=True, download=True, transform=data_transforms['train']),\n",
    "                'val' : datasets.MNIST(root='./data', train=False, download=True, transform=data_transforms['val'])\n",
    "                }\n",
    "    \n",
    "dataloaders_dict = {'train': torch.utils.data.DataLoader(img_datasets['train'], batch_size=64, \n",
    "                                                       shuffle=True, num_workers=4),\n",
    "                    'val': torch.utils.data.DataLoader(img_datasets['val'], batch_size=64, \n",
    "                                                       shuffle=False, num_workers=4)\n",
    "                    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network definition\n",
    "\n",
    "Here we will be defining our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5)\n",
      "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 10, 5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training & Testing\n",
    "\n",
    "Next we will create functions that run training and testing cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    running_corrects = 0\n",
    "    running_loss = 0.0\n",
    "    #optimizer = sgdr(optimizer, epoch+1)\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        _, preds = torch.max(output, 1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_corrects += torch.sum(preds == target.data)\n",
    "        running_loss += loss.item() * target.size(0)\n",
    "    print('Train Epoch: {} Acc: {} Loss: {:.6f}'.format(epoch, running_corrects, running_loss))\n",
    "            \n",
    "def test(model, device, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model():\n",
    "    num_epochs = 20\n",
    "    learning_rate = 3e-4\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for epoch in range(num_epochs):\n",
    "        train(net, device, dataloaders_dict['train'], optimizer, criterion, epoch)\n",
    "        test(net, device, dataloaders_dict['val'], criterion)\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run the model and see, what we get ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 Acc: 43722 Loss: 50006.551396\n",
      "Test set: Average loss: 0.0028, Accuracy: 9493/10000 (94.93%)\n",
      "Train Epoch: 1 Acc: 52880 Loss: 23677.654387\n",
      "Test set: Average loss: 0.0018, Accuracy: 9652/10000 (96.52%)\n",
      "Train Epoch: 2 Acc: 54343 Loss: 19175.770447\n",
      "Test set: Average loss: 0.0015, Accuracy: 9704/10000 (97.04%)\n",
      "Train Epoch: 3 Acc: 54972 Loss: 16829.553546\n",
      "Test set: Average loss: 0.0012, Accuracy: 9762/10000 (97.62%)\n",
      "Train Epoch: 4 Acc: 55577 Loss: 15030.259017\n",
      "Test set: Average loss: 0.0011, Accuracy: 9773/10000 (97.73%)\n",
      "Train Epoch: 5 Acc: 55813 Loss: 14244.480922\n",
      "Test set: Average loss: 0.0010, Accuracy: 9798/10000 (97.98%)\n",
      "Train Epoch: 6 Acc: 56146 Loss: 13305.230465\n",
      "Test set: Average loss: 0.0010, Accuracy: 9804/10000 (98.04%)\n",
      "Train Epoch: 7 Acc: 56256 Loss: 12892.700940\n",
      "Test set: Average loss: 0.0009, Accuracy: 9817/10000 (98.17%)\n",
      "Train Epoch: 8 Acc: 56475 Loss: 12052.413511\n",
      "Test set: Average loss: 0.0009, Accuracy: 9813/10000 (98.13%)\n",
      "Train Epoch: 9 Acc: 56663 Loss: 11667.770246\n",
      "Test set: Average loss: 0.0008, Accuracy: 9833/10000 (98.33%)\n",
      "Train Epoch: 10 Acc: 56792 Loss: 11270.573222\n",
      "Test set: Average loss: 0.0008, Accuracy: 9825/10000 (98.25%)\n",
      "Train Epoch: 11 Acc: 56755 Loss: 11169.012860\n",
      "Test set: Average loss: 0.0008, Accuracy: 9834/10000 (98.34%)\n",
      "Train Epoch: 12 Acc: 56863 Loss: 10755.402521\n",
      "Test set: Average loss: 0.0008, Accuracy: 9826/10000 (98.26%)\n",
      "Train Epoch: 13 Acc: 56978 Loss: 10548.149395\n",
      "Test set: Average loss: 0.0008, Accuracy: 9846/10000 (98.46%)\n",
      "Train Epoch: 14 Acc: 56970 Loss: 10408.446595\n",
      "Test set: Average loss: 0.0008, Accuracy: 9839/10000 (98.39%)\n",
      "Train Epoch: 15 Acc: 57056 Loss: 10149.131011\n",
      "Test set: Average loss: 0.0008, Accuracy: 9842/10000 (98.42%)\n",
      "Train Epoch: 16 Acc: 57062 Loss: 10237.389933\n",
      "Test set: Average loss: 0.0007, Accuracy: 9839/10000 (98.39%)\n",
      "Train Epoch: 17 Acc: 57164 Loss: 10005.786324\n",
      "Test set: Average loss: 0.0007, Accuracy: 9838/10000 (98.38%)\n",
      "Train Epoch: 18 Acc: 57091 Loss: 9860.433884\n",
      "Test set: Average loss: 0.0007, Accuracy: 9847/10000 (98.47%)\n",
      "Train Epoch: 19 Acc: 57077 Loss: 10061.503338\n",
      "Test set: Average loss: 0.0007, Accuracy: 9843/10000 (98.43%)\n"
     ]
    }
   ],
   "source": [
    "run_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
