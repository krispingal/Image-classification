{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image recognition with MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST dataset is one of the most well studied datasets for image recognition.\n",
    "\n",
    "**Objective**\n",
    "Experiment with various deep learning techniques to see it's effects and understand what they mean.\n",
    "\n",
    "I will be iteratively trying something and see it's effects on train accuracy and validation(test) accuracy. This is just an exercise to see how tinkering with each cog would work, and this approach is advisable only for learning purpose. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TL;DR version\n",
    "\n",
    "**Changes made compared to base model**\n",
    "\n",
    "1. Increased epoch number from 10 to 20\n",
    "2. Added more neurons(50) to hidden fc1\n",
    "\n",
    "**Conclusion**\n",
    "1. Noticed that training accuracy dropped in the beginning, but rose to base model's ccuracy at 10 epochs, after 20 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "dtype = torch.float\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in data\n",
    "\n",
    "Let's load in the data. Thankfully torchvision library has inbuilt functions to download the data and load it in.\n",
    "We will be doing only minimal transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train' : transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ]),\n",
    "    'val' : transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "}\n",
    "\n",
    "img_datasets = {'train': datasets.MNIST(root='./data', train=True, download=True, transform=data_transforms['train']),\n",
    "                'val' : datasets.MNIST(root='./data', train=False, download=True, transform=data_transforms['val'])\n",
    "                }\n",
    "    \n",
    "dataloaders_dict = {'train': torch.utils.data.DataLoader(img_datasets['train'], batch_size=64, \n",
    "                                                       shuffle=True, num_workers=4),\n",
    "                    'val': torch.utils.data.DataLoader(img_datasets['val'], batch_size=64, \n",
    "                                                       shuffle=False, num_workers=4)\n",
    "                    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network definition\n",
    "\n",
    "Here we will be defining our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5)\n",
      "  (fc1): Linear(in_features=320, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 10, 5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 100)\n",
    "        self.fc2 = nn.Linear(100, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training & Testing\n",
    "\n",
    "Next we will create functions that run training and testing cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    running_corrects = 0\n",
    "    running_loss = 0.0\n",
    "    #optimizer = sgdr(optimizer, epoch+1)\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        _, preds = torch.max(output, 1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_corrects += torch.sum(preds == target.data)\n",
    "        running_loss += loss.item() * target.size(0)\n",
    "    print('Train Epoch: {} Acc: {} Loss: {:.6f}'.format(epoch, running_corrects, running_loss))\n",
    "            \n",
    "def test(model, device, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.3f}%)'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model():\n",
    "    num_epochs = 20\n",
    "    learning_rate = 3e-4\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=15)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for epoch in range(num_epochs):\n",
    "        train(net, device, dataloaders_dict['train'], optimizer, criterion, epoch)\n",
    "        test(net, device, dataloaders_dict['val'], criterion)\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run the model and see, what we get ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 Acc: 47846 Loss: 39000.449043\n",
      "Test set: Average loss: 0.0023, Accuracy: 9549/10000 (95.490%)\n",
      "Train Epoch: 1 Acc: 55628 Loss: 14947.080315\n",
      "Test set: Average loss: 0.0015, Accuracy: 9709/10000 (97.090%)\n",
      "Train Epoch: 2 Acc: 56596 Loss: 11619.727182\n",
      "Test set: Average loss: 0.0012, Accuracy: 9763/10000 (97.630%)\n",
      "Train Epoch: 3 Acc: 57084 Loss: 10072.432490\n",
      "Test set: Average loss: 0.0010, Accuracy: 9797/10000 (97.970%)\n",
      "Train Epoch: 4 Acc: 57362 Loss: 9284.847289\n",
      "Test set: Average loss: 0.0009, Accuracy: 9815/10000 (98.150%)\n",
      "Train Epoch: 5 Acc: 57560 Loss: 8449.541338\n",
      "Test set: Average loss: 0.0008, Accuracy: 9828/10000 (98.280%)\n",
      "Train Epoch: 6 Acc: 57667 Loss: 7861.029697\n",
      "Test set: Average loss: 0.0008, Accuracy: 9830/10000 (98.300%)\n",
      "Train Epoch: 7 Acc: 57807 Loss: 7538.600097\n",
      "Test set: Average loss: 0.0007, Accuracy: 9847/10000 (98.470%)\n",
      "Train Epoch: 8 Acc: 57904 Loss: 7144.411113\n",
      "Test set: Average loss: 0.0007, Accuracy: 9855/10000 (98.550%)\n",
      "Train Epoch: 9 Acc: 58021 Loss: 6797.583477\n",
      "Test set: Average loss: 0.0007, Accuracy: 9864/10000 (98.640%)\n",
      "Train Epoch: 10 Acc: 58182 Loss: 6412.299541\n",
      "Test set: Average loss: 0.0007, Accuracy: 9859/10000 (98.590%)\n",
      "Train Epoch: 11 Acc: 58199 Loss: 6204.153122\n",
      "Test set: Average loss: 0.0006, Accuracy: 9860/10000 (98.600%)\n",
      "Train Epoch: 12 Acc: 58138 Loss: 6325.018068\n",
      "Test set: Average loss: 0.0007, Accuracy: 9861/10000 (98.610%)\n",
      "Train Epoch: 13 Acc: 58245 Loss: 6187.165675\n",
      "Test set: Average loss: 0.0006, Accuracy: 9865/10000 (98.650%)\n",
      "Train Epoch: 14 Acc: 58294 Loss: 5922.217921\n",
      "Test set: Average loss: 0.0006, Accuracy: 9864/10000 (98.640%)\n",
      "Train Epoch: 15 Acc: 58283 Loss: 5976.305983\n",
      "Test set: Average loss: 0.0006, Accuracy: 9865/10000 (98.650%)\n",
      "Train Epoch: 16 Acc: 58304 Loss: 5857.297317\n",
      "Test set: Average loss: 0.0006, Accuracy: 9865/10000 (98.650%)\n",
      "Train Epoch: 17 Acc: 58250 Loss: 5977.665388\n",
      "Test set: Average loss: 0.0006, Accuracy: 9866/10000 (98.660%)\n",
      "Train Epoch: 18 Acc: 58322 Loss: 5906.520357\n",
      "Test set: Average loss: 0.0006, Accuracy: 9866/10000 (98.660%)\n",
      "Train Epoch: 19 Acc: 58219 Loss: 6137.954557\n",
      "Test set: Average loss: 0.0006, Accuracy: 9867/10000 (98.670%)\n"
     ]
    }
   ],
   "source": [
    "run_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
