{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image recognition with MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST dataset is one of the most well studied datasets for image recognition.\n",
    "\n",
    "**Objective**\n",
    "Experiment with various deep learning techniques to see it's effects and understand what they mean.\n",
    "\n",
    "I will be iteratively trying something and see it's effects on train accuracy and validation(test) accuracy. This is just an exercise to see how tinkering with each cog would work, and this approach is advisable only for learning purpose. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "dtype = torch.float\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in data\n",
    "\n",
    "Let's load in the data. Thankfully torchvision library has inbuilt functions to download the data and load it in.\n",
    "We will be doing only minimal transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train' : transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ]),\n",
    "    'val' : transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "}\n",
    "\n",
    "img_datasets = {'train': datasets.MNIST(root='./data', train=True, download=True, transform=data_transforms['train']),\n",
    "                'val' : datasets.MNIST(root='./data', train=False, download=True, transform=data_transforms['val'])\n",
    "                }\n",
    "    \n",
    "dataloaders_dict = {'train': torch.utils.data.DataLoader(img_datasets['train'], batch_size=64, \n",
    "                                                       shuffle=True, num_workers=4),\n",
    "                    'val': torch.utils.data.DataLoader(img_datasets['val'], batch_size=64, \n",
    "                                                       shuffle=False, num_workers=4)\n",
    "                    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network definition\n",
    "\n",
    "Here we will be defining our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5)\n",
      "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 10, 5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training & Testing\n",
    "\n",
    "Next we will create functions that run training and testing cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    running_corrects = 0\n",
    "    running_loss = 0.0\n",
    "    #optimizer = sgdr(optimizer, epoch+1)\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        _, preds = torch.max(output, 1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_corrects += torch.sum(preds == target.data)\n",
    "        running_loss += loss.item() * target.size(0)\n",
    "    print('Train Epoch: {} Acc: {} Loss: {:.6f}'.format(epoch, running_corrects, running_loss))\n",
    "            \n",
    "def test(model, device, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model():\n",
    "    num_epochs = 10\n",
    "    learning_rate = 3e-4\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=7)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for epoch in range(num_epochs):\n",
    "        train(net, device, dataloaders_dict['train'], optimizer, criterion, epoch)\n",
    "        test(net, device, dataloaders_dict['train'], criterion)\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run the model and see, what we get ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 Acc: 43226 Loss: 51337.739029\n",
      "Test set: Average loss: 0.0031, Accuracy: 56594/60000 (94%)\n",
      "Train Epoch: 1 Acc: 53712 Loss: 21470.091945\n",
      "Test set: Average loss: 0.0019, Accuracy: 57897/60000 (96%)\n",
      "Train Epoch: 2 Acc: 55364 Loss: 16355.423492\n",
      "Test set: Average loss: 0.0016, Accuracy: 58174/60000 (97%)\n",
      "Train Epoch: 3 Acc: 55922 Loss: 14051.841046\n",
      "Test set: Average loss: 0.0013, Accuracy: 58505/60000 (98%)\n",
      "Train Epoch: 4 Acc: 56279 Loss: 12812.538852\n",
      "Test set: Average loss: 0.0012, Accuracy: 58613/60000 (98%)\n",
      "Train Epoch: 5 Acc: 56475 Loss: 12251.675348\n",
      "Test set: Average loss: 0.0011, Accuracy: 58715/60000 (98%)\n",
      "Train Epoch: 6 Acc: 56771 Loss: 11447.955053\n",
      "Test set: Average loss: 0.0011, Accuracy: 58719/60000 (98%)\n",
      "Train Epoch: 7 Acc: 56753 Loss: 11249.068364\n",
      "Test set: Average loss: 0.0011, Accuracy: 58738/60000 (98%)\n",
      "Train Epoch: 8 Acc: 56802 Loss: 11273.660331\n",
      "Test set: Average loss: 0.0011, Accuracy: 58738/60000 (98%)\n",
      "Train Epoch: 9 Acc: 56830 Loss: 11053.372862\n",
      "Test set: Average loss: 0.0011, Accuracy: 58749/60000 (98%)\n"
     ]
    }
   ],
   "source": [
    "run_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
