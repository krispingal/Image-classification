{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image recognition with MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST dataset is one of the most well studied datasets for image recognition.\n",
    "\n",
    "**Objective**\n",
    "Experiment with various deep learning techniques to see it's effects and understand what they mean.\n",
    "\n",
    "I will be iteratively trying something and see it's effects on train accuracy and validation(test) accuracy. This is just an exercise to see how tinkering with each cog would work, and this approach is advisable only for learning purpose. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TL;DR version\n",
    "\n",
    "**Changes made compared to base model**\n",
    "\n",
    "1. Increased epoch number from 10 to 20\n",
    "2. Added transformation for random rotation\n",
    "\n",
    "**Conclusion**\n",
    "1. Noticed that training accuracy dropped in the beginning, but rose to base model's ccuracy at 10 epochs, after 20 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "dtype = torch.float\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in data\n",
    "\n",
    "Let's load in the data. Thankfully torchvision library has inbuilt functions to download the data and load it in.\n",
    "We will be doing only minimal transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train' : transforms.Compose(\n",
    "    [transforms.RandomRotation(20),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ]),\n",
    "    'val' : transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "}\n",
    "\n",
    "img_datasets = {'train': datasets.MNIST(root='./data', train=True, download=True, transform=data_transforms['train']),\n",
    "                'val' : datasets.MNIST(root='./data', train=False, download=True, transform=data_transforms['val'])\n",
    "                }\n",
    "    \n",
    "dataloaders_dict = {'train': torch.utils.data.DataLoader(img_datasets['train'], batch_size=64, \n",
    "                                                       shuffle=True, num_workers=4),\n",
    "                    'val': torch.utils.data.DataLoader(img_datasets['val'], batch_size=64, \n",
    "                                                       shuffle=False, num_workers=4)\n",
    "                    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network definition\n",
    "\n",
    "Here we will be defining our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5)\n",
      "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 10, 5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training & Testing\n",
    "\n",
    "Next we will create functions that run training and testing cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    running_corrects = 0\n",
    "    running_loss = 0.0\n",
    "    #optimizer = sgdr(optimizer, epoch+1)\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        _, preds = torch.max(output, 1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_corrects += torch.sum(preds == target.data)\n",
    "        running_loss += loss.item() * target.size(0)\n",
    "    print('Train Epoch: {} Acc: {} Loss: {:.6f}'.format(epoch, running_corrects, running_loss))\n",
    "            \n",
    "def test(model, device, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model():\n",
    "    num_epochs = 20\n",
    "    learning_rate = 3e-4\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=7)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for epoch in range(num_epochs):\n",
    "        train(net, device, dataloaders_dict['train'], optimizer, criterion, epoch)\n",
    "        test(net, device, dataloaders_dict['train'], criterion)\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run the model and see, what we get ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 Acc: 42486 Loss: 54334.084827\n",
      "Test set: Average loss: 0.0039, Accuracy: 55797/60000 (93%)\n",
      "Train Epoch: 1 Acc: 52208 Loss: 26023.474498\n",
      "Test set: Average loss: 0.0026, Accuracy: 56989/60000 (95%)\n",
      "Train Epoch: 2 Acc: 53896 Loss: 20713.435229\n",
      "Test set: Average loss: 0.0021, Accuracy: 57468/60000 (96%)\n",
      "Train Epoch: 3 Acc: 54582 Loss: 18394.412598\n",
      "Test set: Average loss: 0.0019, Accuracy: 57858/60000 (96%)\n",
      "Train Epoch: 4 Acc: 55122 Loss: 16393.656138\n",
      "Test set: Average loss: 0.0018, Accuracy: 57872/60000 (96%)\n",
      "Train Epoch: 5 Acc: 55311 Loss: 16084.132507\n",
      "Test set: Average loss: 0.0017, Accuracy: 57985/60000 (97%)\n",
      "Train Epoch: 6 Acc: 55571 Loss: 15434.033919\n",
      "Test set: Average loss: 0.0016, Accuracy: 58076/60000 (97%)\n",
      "Train Epoch: 7 Acc: 55545 Loss: 15368.197688\n",
      "Test set: Average loss: 0.0016, Accuracy: 58109/60000 (97%)\n",
      "Train Epoch: 8 Acc: 55627 Loss: 14952.036166\n",
      "Test set: Average loss: 0.0016, Accuracy: 58117/60000 (97%)\n",
      "Train Epoch: 9 Acc: 55792 Loss: 14707.972164\n",
      "Test set: Average loss: 0.0016, Accuracy: 58131/60000 (97%)\n",
      "Train Epoch: 10 Acc: 55700 Loss: 14855.337546\n",
      "Test set: Average loss: 0.0016, Accuracy: 58162/60000 (97%)\n",
      "Train Epoch: 11 Acc: 55680 Loss: 14754.269248\n",
      "Test set: Average loss: 0.0015, Accuracy: 58222/60000 (97%)\n",
      "Train Epoch: 12 Acc: 55752 Loss: 14750.196261\n",
      "Test set: Average loss: 0.0014, Accuracy: 58262/60000 (97%)\n",
      "Train Epoch: 13 Acc: 55856 Loss: 14237.516980\n",
      "Test set: Average loss: 0.0014, Accuracy: 58325/60000 (97%)\n",
      "Train Epoch: 14 Acc: 56045 Loss: 13704.771105\n",
      "Test set: Average loss: 0.0014, Accuracy: 58367/60000 (97%)\n",
      "Train Epoch: 15 Acc: 56054 Loss: 13518.857103\n",
      "Test set: Average loss: 0.0013, Accuracy: 58397/60000 (97%)\n",
      "Train Epoch: 16 Acc: 56215 Loss: 12827.150422\n",
      "Test set: Average loss: 0.0012, Accuracy: 58535/60000 (98%)\n",
      "Train Epoch: 17 Acc: 56359 Loss: 12413.742512\n",
      "Test set: Average loss: 0.0012, Accuracy: 58597/60000 (98%)\n",
      "Train Epoch: 18 Acc: 56476 Loss: 12087.657008\n",
      "Test set: Average loss: 0.0012, Accuracy: 58581/60000 (98%)\n",
      "Train Epoch: 19 Acc: 56620 Loss: 11823.368263\n",
      "Test set: Average loss: 0.0011, Accuracy: 58646/60000 (98%)\n"
     ]
    }
   ],
   "source": [
    "run_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
